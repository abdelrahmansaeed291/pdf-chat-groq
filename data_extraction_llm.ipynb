{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b723282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 0) Install dependencies (run once per environment)\n",
    "%pip install -U pip\n",
    "%pip install -U langchain langchain-community langchain-core  langchain-groq sentence-transformers chromadb pypdf python-dotenv\n",
    "%pip install -U ipywidgets\n",
    "%pip install -U jupyter\n",
    "%pip install -U ipywidgets tqdm\n",
    "%pip install -U widgetsnbextension\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10baf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has GROQ key? True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv; load_dotenv()\n",
    "import os; print(\"Has GROQ key?\", bool(os.getenv(\"GROQ_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "acd6a723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.1/11.3 MB 11.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.0/11.3 MB 12.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.6/11.3 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.2/11.3 MB 12.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 11.3 MB/s  0:00:01\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   ---------------------------------------- 3/3 [pandas]\n",
      "\n",
      "Successfully installed pandas-2.3.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import os\n",
    "import tempfile\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv \n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e10044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports & config\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load .env (expects GROQ_API_KEY)\n",
    "load_dotenv()  # or load_dotenv(dotenv_path=\"C:/path/to/.env\")\n",
    "assert os.getenv(\"GROQ_API_KEY\"), \"GROQ_API_KEY not found. Add it to your .env.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f22903f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets>=8\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tqdm>=4.66 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: ipykernel>=6.25 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (6.30.1)\n",
      "Requirement already satisfied: traitlets>=5.10 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipywidgets>=8) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipywidgets>=8) (9.6.0)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=8)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets>=8)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from tqdm>=4.66) (0.4.6)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipykernel>=6.25) (1.8.17)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipykernel>=6.25) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipykernel>=6.25) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipykernel>=6.25) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipykernel>=6.25) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipykernel>=6.25) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipykernel>=6.25) (7.1.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipykernel>=6.25) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipykernel>=6.25) (6.5.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=8) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8) (0.8.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from jupyter-client>=8.0.0->ipykernel>=6.25) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.25) (4.4.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.25) (311)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel>=6.25) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=8) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.3 MB/s  0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   ---------------------------------------- 3/3 [ipywidgets]\n",
      "\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U \"ipywidgets>=8\" \"tqdm>=4.66\" \"ipykernel>=6.25\" \"traitlets>=5.10\"\n",
    "# Only needed for classic Jupyter Notebook in a browser, NOT VS Code:\n",
    "# %pip install -U widgetsnbextension==4.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f08e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Desktop\\RAG-System\\rag_llms\\myenv311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for _ in tqdm(range(100)):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f6c03",
   "metadata": {},
   "source": [
    "## Define our LLM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c368b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab525da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()  # loads GROQ_API_KEY from your .env\n",
    "key = os.getenv(\"GROQ_API_KEY\")\n",
    "assert key and key.startswith(\"gsk_\"), \"GROQ_API_KEY missing or malformed.\"\n",
    "\n",
    "# Either pass it explicitly...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23f86b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "# LLM (Groq) ---------------------------------------------------------------\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "import os\n",
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "assert os.getenv(\"GROQ_API_KEY\"), \"Put GROQ_API_KEY in your .env\"\n",
    "\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "available = {m.id for m in client.models.list().data}\n",
    "\n",
    "PREFERRED = [\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    \"gemma2-9b-it\",\n",
    "]\n",
    "model_name = next((m for m in PREFERRED if m in available), None)\n",
    "if not model_name:\n",
    "    raise RuntimeError(f\"No preferred Groq models enabled. Have: {sorted(available)}\")\n",
    "\n",
    "llm = ChatGroq(model=model_name)\n",
    "print(\"Using LLM:\", model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c11bdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello to you' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 42, 'total_tokens': 46, 'completion_time': 0.025212753, 'prompt_time': 0.001865466, 'queue_time': 0.090877568, 'total_time': 0.027078219}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_155ab82e98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--fc25e7bd-6427-4ac2-8f38-60d080248327-0' usage_metadata={'input_tokens': 42, 'output_tokens': 4, 'total_tokens': 46}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the cat join a band? Because it wanted to be the purr-cussionist.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 41, 'total_tokens': 62, 'completion_time': 0.039433395, 'prompt_time': 0.002073381, 'queue_time': 0.091820913, 'total_time': 0.041506776}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_155ab82e98', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--82657350-56de-48a6-8bba-e7dba6a36d56-0', usage_metadata={'input_tokens': 41, 'output_tokens': 21, 'total_tokens': 62})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")  # relies on GROQ_API_KEY in env/.env\n",
    "print(llm.invoke(\"Say hi in 3 words.\"))\n",
    "llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaa3d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PDF_PATH = \"data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf\"  # <-- change to your PDF filename\n",
    "CHUNK_SIZE = 1500\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c63cc33",
   "metadata": {},
   "source": [
    "## Process PDF document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7eaee",
   "metadata": {},
   "source": [
    "### Load PDF Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f06ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 18 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pages: 3 | Chunks: 9\n"
     ]
    }
   ],
   "source": [
    "# 3) Load PDF and split into chunks\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "pages = loader.load()\n",
    "pages\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
    "                                            chunk_overlap=200,\n",
    "                                            length_function=len,\n",
    "                                            separators=[\"\\n\\n\", \"\\n\", \" \"])\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "chunks\n",
    "\n",
    "print(f\"Loaded pages: {len(pages)} | Chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb4ee8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='APPLIED COGNITIVE PSYCHOLOGY\\nAppl. Cognit. Psychol. 20: 139–156 (2006)\\nPublished online 31 October 2005 in Wiley InterScience\\n(www.interscience.wiley.com) DOI: 10.1002/acp.1178\\nConsequences of Erudite Vernacular Utilized Irrespective\\nof Necessity: Problems with Using Long Words Needlessly\\nDANIEL M. OPPENHEIMER*\\nPrinceton University, USA\\nSUMMARY\\nMost texts on writing style encourage authors to avoid overly-complex words. However, a majority\\nof undergraduates admit to deliberately increasing the complexity of their vocabulary so as to give\\nthe impression of intelligence. This paper explores the extent to which this strategy is effective.\\nExperiments 1–3 manipulate complexity of texts and ﬁnd a negative relationship between complex-\\nity and judged intelligence. This relationship held regardless of the quality of the original essay, and\\nirrespective of the participants’ prior expectations of essay quality. The negative impact of\\ncomplexity was mediated by processing ﬂuency. Experiment 4 directly manipulated ﬂuency and\\nfound that texts in hard to read fonts are judged to come from less intelligent authors. Experiment 5\\ninvestigated discounting of ﬂuency. When obvious causes for low ﬂuency exist that are not relevant\\nto the judgement at hand, people reduce their reliance on ﬂuency as a cue; in fact, in an effort not to\\nbe inﬂuenced by the irrelevant source of ﬂuency, they over-compensate and are biased in the opposite\\ndirection. Implications and applications are discussed. Copyright # 2005 John Wiley & Sons, Ltd.\\nWhen it comes to writing, most experts agree that clarity, simplicity and parsimony are\\nideals that authors should strive for. In their classic manual of style, Strunk and White\\n(1979) encourage authors to ‘omit needless words.’ Daryl Bem’s (1995) guidelines for\\nsubmission to Psychological Bulletin advise, ‘the ﬁrst step towards clarity is writing\\nsimply.’ Even the APA publication manual (1996) recommends, ‘direct, declarative\\nsentences with simple common words are usually best.’\\nHowever, most of us can likely recall having read papers, either by colleagues or\\nstudents, in which the author appears to be deliberately using overly complex words.\\nExperience suggests that the experts’ advice contrasts with prevailing wisdom on how to\\nsound more intelligent as a writer. In fact, when 110 Stanford undergraduates were polled\\nabout their writing habits, most of them admitted that they had made their writing more\\ncomplex in order to appear smarter. For example, when asked, ‘Have you ever changed the\\nwords in an academic essay to make the essay sound more valid or intelligent by using\\ncomplicated language?’ 86.4% of the sample admitted to having done so. Nearly two-\\nthirds answered yes to the question, ‘When you write an essay, do you turn to the thesaurus\\nto choose words that are more complex to give the impression that the content is more\\nvalid or intelligent?’\\nCopyright # 2005 John Wiley & Sons, Ltd.\\n*Correspondence to: D. M. Oppenheimer, Department of Psychology, Princeton University, Green Hall Room\\n2-S-8, Princeton, NJ 08540, USA. E-mail: doppenhe@princeton.edu'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='There are many plausible reasons that the use of million-dollar words would lead\\nreaders to believe that an author is smart. Intelligence and large vocabularies are positively\\ncorrelated (Spearman, 1904). Therefore, by displaying a large vocabulary, one may be\\nproviding cues that he or she is intelligent as well. Secondly, writers are assumed to be\\nconforming to the Gricean maxim of manner, ‘avoid obscurity of expression’ (Grice,\\n1975). If authors are believed to be writing as simply as possible, but a text is nonetheless\\ncomplex, a reader might believe that the ideas expressed in that text are also complex,\\ndefying all attempts to simplify the language. Further, individuals forced to struggle\\nthrough a complex text might experience dissonance if they believe that the ideas being\\nconveyed are simple (Festinger, 1957). Thus, individuals might be motivated to perceive a\\ndifﬁcult text as being more worthwhile, thereby justifying the effort of processing.\\nIndeed, there is some evidence that complex vocabulary can be indicative of a more\\nintelligent author. For example, Penneba ker and King (1999) have shown that the\\npercentage of long words used in class as signments positively correlates with SAT\\nscores and exam grades on both multiple cho ice and essay tests. However it is difﬁcult\\nto draw conclusions about the effectiveness of a strategy of complexity from this data.\\nThe study did not look at how readers of the texts containing the long words perceived\\nthe authors’ intelligence. Thus, it is possi ble that although students using complex\\nvocabularies are objectively very knowledgeable, they might nonetheless be perceived\\nas being less so.\\nWhy might we believe that the experts might be correct in recommending simplicity in\\nwriting? One theory that predicts the effectiveness of straightforward writing is that of\\nprocessing ﬂuency. Simpler writing is easier to process, and studies have demonstrated\\nthat processing ﬂuency is associated with a variety of positive dimensions. Fluency leads\\nto higher judgements of truth (Reber & Schwarz, 1999), conﬁdence (Norwick & Epley,\\n2002), frequency (Tversky & Kahneman, 1973), fame (Jacoby, Kelley, Brown, &\\nJasechko, 1989), and even liking (Reber, Winkielman, & Schwarz, 1998). Furthermore,\\nthe effects of ﬂuency are strongest when the ﬂuency is discrepant—when the amount of\\nexperienced ﬂuency is surprising (Whittlesea & Williams, 2001a, 2001b). As such, it\\nwould not be surprising if the lower ﬂuency of overly complex texts caused readers to have\\nnegative evaluations of those texts and the associated authors, especially if the complexity\\nwas unnecessary and thus surprising readers with the relative disﬂuency of the text.\\nBoth the experts and prevailing wisdom present plausible views, but which (if either) is\\ncorrect? The present paper provides an empirical investigation of the strategy of complex-\\nity, and ﬁnds such a strategy to be unsuccessful. Five studies demonstrate that the loss of\\nﬂuency due to needless complexity in a text negatively impacts raters’ assessments of the\\ntext’s authors.\\nEXPERIMENT 1\\nExperiment 1 aimed to answer several simple questions. First, does increasing the\\ncomplexity of text succeed in making the author appear more intelligent? Second, to\\nwhat extent does the success of this strategy depend on the quality of the original, simpler\\nwriting? Finally, if the strategy is unsuccessful, is the failure of the strategy due to loss of\\nﬂuency? To answer these questions, graduate school admission essays were made more\\ncomplex by substituting some of the original words with their longest applicable thesaurus\\nentries.\\n140 D. M. Oppenheimer\\nCopyright # 2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139–156 (2006)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='While word length is not perfectly interchangeable with sentence complexity—for\\nexample, complexity can come from grammatical structure or infrequent words as\\nwell—it is a useful proxy. Using length as a manipulation of complexity allows for a\\nsimple, easily replicable word replacement algorithm. By keeping content constant and\\nvarying the complexity of vocabulary, it was possible to investigate the effectiveness of\\ncomplexity.\\nParticipants and procedure\\nSeventy-one Stanford University undergraduates participated to fulﬁl part of a course\\nrequirement. The survey was included in a packet of unrelated one-page questionnaires.\\nPackets were distributed in class, and participants were given a week to complete the entire\\npacket.\\nStimuli and design\\nSix personal statements for admissions to graduate studies in English Literature were\\ndownloaded from writing improvement websites. The essays varied greatly both in content\\nand quality of writing. Logical excerpts ranging from 138 to 253 words in length were then\\ntaken from each essay. A ‘highly complex’ version of each excerpt was prepared by\\nreplacing every noun, verb and adjective with its longest entry in the Microsoft Word 2000\\nthesaurus. Words that were longer than any thesaurus entry, were not listed in the\\nthesaurus, or for which there was no entry with the same linguistic sense were not\\nreplaced. If two entries were of the same length, the replacement was chosen alphabe-\\ntically. When necessary, minor modiﬁcations were made to the essay to maintain the\\ngrammatical structure of a sentence (e.g. replacing ‘an’ with ‘a’ for replacement words\\nbeginning with consonants). A ‘moderately complex’ version of each excerpt was created\\nusing the same algorithm as above, except replacing only every third applicable word.\\nExamples of the stimuli can be found in the appendix.\\nEach participant received only one excerpt. Participants were informed that the excerpt\\ncame from a personal statement for graduate study in the Stanford English department.\\nThey were instructed to read the passage, decide whether or not to accept the applicant,\\nand rate their conﬁdence in their decision on a 7-point scale. 1 They were then asked how\\ndifﬁcult the passage was to understand, also on a seven-point scale.\\nResults\\nThe data of one participant was discarded due to an illegible answer. Analysis of the\\nmanipulation check showed that more complex texts were more difﬁcult to read. ( x ¼ 2.9,\\n4.0 and 4.3 for simple, moderately complex and highly complex, respectively). These\\ndifferences were reliable, F(2, 68) ¼ 4.46, p < 0.05, Cohen’s f ¼ 0.18. For other analyses,\\nacceptance ratings ( þ1 for accept, /C01 for reject) were multiplied by conﬁdence ratings to\\ncreate a /C07 to 7 scale of admission conﬁdence. Level of complexity had a reliable\\ninﬂuence on admission conﬁdence ratings, F(2, 70) ¼ 2.46, p < 0.05, Cohen’s f ¼ 0.12.\\n1With the exception of the dichotomous admissions decision, all dependent measures reported in this paper are\\nseven point scales ranging from 1 ¼ ‘not at all’ to 7¼ ‘very’.\\nProblems with long words 141\\nCopyright # 2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139–156 (2006)')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='APPLIED COGNITIVE PSYCHOLOGY\\nAppl. Cognit. Psychol. 20: 139–156 (2006)\\nPublished online 31 October 2005 in Wiley InterScience\\n(www.interscience.wiley.com) DOI: 10.1002/acp.1178\\nConsequences of Erudite Vernacular Utilized Irrespective\\nof Necessity: Problems with Using Long Words Needlessly\\nDANIEL M. OPPENHEIMER*\\nPrinceton University, USA\\nSUMMARY\\nMost texts on writing style encourage authors to avoid overly-complex words. However, a majority\\nof undergraduates admit to deliberately increasing the complexity of their vocabulary so as to give\\nthe impression of intelligence. This paper explores the extent to which this strategy is effective.\\nExperiments 1–3 manipulate complexity of texts and ﬁnd a negative relationship between complex-\\nity and judged intelligence. This relationship held regardless of the quality of the original essay, and\\nirrespective of the participants’ prior expectations of essay quality. The negative impact of\\ncomplexity was mediated by processing ﬂuency. Experiment 4 directly manipulated ﬂuency and\\nfound that texts in hard to read fonts are judged to come from less intelligent authors. Experiment 5\\ninvestigated discounting of ﬂuency. When obvious causes for low ﬂuency exist that are not relevant\\nto the judgement at hand, people reduce their reliance on ﬂuency as a cue; in fact, in an effort not to\\nbe inﬂuenced by the irrelevant source of ﬂuency, they over-compensate and are biased in the opposite'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='be inﬂuenced by the irrelevant source of ﬂuency, they over-compensate and are biased in the opposite\\ndirection. Implications and applications are discussed. Copyright # 2005 John Wiley & Sons, Ltd.\\nWhen it comes to writing, most experts agree that clarity, simplicity and parsimony are\\nideals that authors should strive for. In their classic manual of style, Strunk and White\\n(1979) encourage authors to ‘omit needless words.’ Daryl Bem’s (1995) guidelines for\\nsubmission to Psychological Bulletin advise, ‘the ﬁrst step towards clarity is writing\\nsimply.’ Even the APA publication manual (1996) recommends, ‘direct, declarative\\nsentences with simple common words are usually best.’\\nHowever, most of us can likely recall having read papers, either by colleagues or\\nstudents, in which the author appears to be deliberately using overly complex words.\\nExperience suggests that the experts’ advice contrasts with prevailing wisdom on how to\\nsound more intelligent as a writer. In fact, when 110 Stanford undergraduates were polled\\nabout their writing habits, most of them admitted that they had made their writing more\\ncomplex in order to appear smarter. For example, when asked, ‘Have you ever changed the\\nwords in an academic essay to make the essay sound more valid or intelligent by using\\ncomplicated language?’ 86.4% of the sample admitted to having done so. Nearly two-\\nthirds answered yes to the question, ‘When you write an essay, do you turn to the thesaurus'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='complicated language?’ 86.4% of the sample admitted to having done so. Nearly two-\\nthirds answered yes to the question, ‘When you write an essay, do you turn to the thesaurus\\nto choose words that are more complex to give the impression that the content is more\\nvalid or intelligent?’\\nCopyright # 2005 John Wiley & Sons, Ltd.\\n*Correspondence to: D. M. Oppenheimer, Department of Psychology, Princeton University, Green Hall Room\\n2-S-8, Princeton, NJ 08540, USA. E-mail: doppenhe@princeton.edu'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='There are many plausible reasons that the use of million-dollar words would lead\\nreaders to believe that an author is smart. Intelligence and large vocabularies are positively\\ncorrelated (Spearman, 1904). Therefore, by displaying a large vocabulary, one may be\\nproviding cues that he or she is intelligent as well. Secondly, writers are assumed to be\\nconforming to the Gricean maxim of manner, ‘avoid obscurity of expression’ (Grice,\\n1975). If authors are believed to be writing as simply as possible, but a text is nonetheless\\ncomplex, a reader might believe that the ideas expressed in that text are also complex,\\ndefying all attempts to simplify the language. Further, individuals forced to struggle\\nthrough a complex text might experience dissonance if they believe that the ideas being\\nconveyed are simple (Festinger, 1957). Thus, individuals might be motivated to perceive a\\ndifﬁcult text as being more worthwhile, thereby justifying the effort of processing.\\nIndeed, there is some evidence that complex vocabulary can be indicative of a more\\nintelligent author. For example, Penneba ker and King (1999) have shown that the\\npercentage of long words used in class as signments positively correlates with SAT\\nscores and exam grades on both multiple cho ice and essay tests. However it is difﬁcult\\nto draw conclusions about the effectiveness of a strategy of complexity from this data.\\nThe study did not look at how readers of the texts containing the long words perceived'),\n",
       " Document(metadata={'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'author': 'Thu Vu', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='to draw conclusions about the effectiveness of a strategy of complexity from this data.\\nThe study did not look at how readers of the texts containing the long words perceived\\nthe authors’ intelligence. Thus, it is possi ble that although students using complex\\nvocabularies are objectively very knowledgeable, they might nonetheless be perceived\\nas being less so.\\nWhy might we believe that the experts might be correct in recommending simplicity in\\nwriting? One theory that predicts the effectiveness of straightforward writing is that of\\nprocessing ﬂuency. Simpler writing is easier to process, and studies have demonstrated\\nthat processing ﬂuency is associated with a variety of positive dimensions. Fluency leads\\nto higher judgements of truth (Reber & Schwarz, 1999), conﬁdence (Norwick & Epley,\\n2002), frequency (Tversky & Kahneman, 1973), fame (Jacoby, Kelley, Brown, &\\nJasechko, 1989), and even liking (Reber, Winkielman, & Schwarz, 1998). Furthermore,\\nthe effects of ﬂuency are strongest when the ﬂuency is discrepant—when the amount of\\nexperienced ﬂuency is surprising (Whittlesea & Williams, 2001a, 2001b). As such, it\\nwould not be surprising if the lower ﬂuency of overly complex texts caused readers to have\\nnegative evaluations of those texts and the associated authors, especially if the complexity\\nwas unnecessary and thus surprising readers with the relative disﬂuency of the text.\\nBoth the experts and prevailing wisdom present plausible views, but which (if either) is')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pages: 3 | Chunks: 9\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(pages[:3])   # preview first 3 documents\n",
    "display(chunks[:5]) # preview first 5 chunks\n",
    "print(f\"Loaded pages: {len(pages)} | Chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a852e06",
   "metadata": {},
   "source": [
    "## Create Embedings \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c71d460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.8.0-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached regex-2025.9.18-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached markupsafe-3.0.3-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.5)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lenovo\\desktop\\rag-system\\rag_llms\\myenv311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Using cached sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Using cached transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "Using cached regex-2025.9.18-cp311-cp311-win_amd64.whl (276 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Using cached torch-2.8.0-cp311-cp311-win_amd64.whl (241.4 MB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached scikit_learn-1.7.2-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Installing collected packages: safetensors, regex, networkx, MarkupSafe, joblib, scikit-learn, jinja2, torch, transformers, sentence-transformers\n",
      "\n",
      "   ---- -----------------------------------  1/10 [regex]\n",
      "   ---- -----------------------------------  1/10 [regex]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   -------- -------------------------------  2/10 [networkx]\n",
      "   ---------------- -----------------------  4/10 [joblib]\n",
      "   ---------------- -----------------------  4/10 [joblib]\n",
      "   ---------------- -----------------------  4/10 [joblib]\n",
      "   ---------------- -----------------------  4/10 [joblib]\n",
      "   ---------------- -----------------------  4/10 [joblib]\n",
      "   ---------------- -----------------------  4/10 [joblib]\n",
      "   ---------------- -----------------------  4/10 [joblib]\n",
      "   ---------------- -----------------------  4/10 [joblib]\n",
      "   ---------------- -----------------------  4/10 [joblib]\n",
      "   ---------------- -----------------------  4/10 [joblib]\n",
      "   ---------------- -----------------------  4/10 [joblib]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   -------------------- -------------------  5/10 [scikit-learn]\n",
      "   ------------------------ ---------------  6/10 [jinja2]\n",
      "   ------------------------ ---------------  6/10 [jinja2]\n",
      "   ------------------------ ---------------  6/10 [jinja2]\n",
      "   ------------------------ ---------------  6/10 [jinja2]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   ---------------------------- -----------  7/10 [torch]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   -------------------------------- -------  8/10 [transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ------------------------------------ ---  9/10 [sentence-transformers]\n",
      "   ---------------------------------------- 10/10 [sentence-transformers]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.3 jinja2-3.1.6 joblib-1.5.2 networkx-3.5 regex-2025.9.18 safetensors-0.6.2 scikit-learn-1.7.2 sentence-transformers-5.1.1 torch-2.8.0 transformers-4.57.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentence-transformers\n",
    "# If pip doesn't pull PyTorch automatically, install the CPU build explicitly:\n",
    "# %pip install -U torch --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6a14977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim: 384 | first 8: [0.03733031079173088, 0.051161788403987885, -0.0003060445887967944, 0.060209885239601135, -0.11749441176652908, -0.014230075292289257, 0.10577617585659027, 0.02678624354302883]\n"
     ]
    }
   ],
   "source": [
    "# pip install -U sentence-transformers langchain-community\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def get_embedding_function(\n",
    "    embed_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",  # embedding model (not the LLM)\n",
    "):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=embed_model,\n",
    "        encode_kwargs={\"normalize_embeddings\": True},   # cosine-normalized vectors\n",
    "    )\n",
    "\n",
    "embedding_function = get_embedding_function()\n",
    "test_vector = embedding_function.embed_query(\"cat\")\n",
    "print(\"Dim:\", len(test_vector), \"| first 8:\", test_vector[:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10146c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7261297861891461}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(evaluator=\"embedding_distance\", \n",
    "                            embeddings=embedding_function)\n",
    "\n",
    "evaluator.evaluate_strings(prediction=\"Amsterdam\", reference=\"coffeeshop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "382b4c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7031169469617871}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_strings(prediction=\"Paris\", reference=\"coffeeshop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15febbd3",
   "metadata": {},
   "source": [
    "### Create Vector database \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "420e2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "\n",
    "    # Create a list of unique ids for each document based on the content\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]\n",
    "    \n",
    "    # Ensure that only unique docs with unique ids are kept\n",
    "    unique_ids = set()\n",
    "    unique_chunks = []\n",
    "    \n",
    "    unique_chunks = [] \n",
    "    for chunk, id in zip(chunks, ids):     \n",
    "        if id not in unique_ids:       \n",
    "            unique_ids.add(id)\n",
    "            unique_chunks.append(chunk) \n",
    "\n",
    "    # Create a new Chroma database from the documents\n",
    "    vectorstore = Chroma.from_documents(documents=unique_chunks, \n",
    "                                        ids=list(unique_ids),\n",
    "                                        embedding=embedding_function, \n",
    "                                        persist_directory = vectorstore_path)\n",
    "\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4dcf9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorstore\n",
    "vectorstore = create_vectorstore(chunks=chunks, \n",
    "                                 embedding_function=embedding_function, \n",
    "                                 vectorstore_path=\"vectorstore_chroma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7bf00",
   "metadata": {},
   "source": [
    "## 2. Query for relevant data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb5779ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectorstore\n",
    "vectorstore = Chroma(persist_directory=\"vectorstore_chroma\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d29207d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page_label': '2', 'page': 1, 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'moddate': \"D:20240910141854Z00'00'\", 'total_pages': 3, 'author': 'Thu Vu', 'creator': 'Preview', 'creationdate': \"D:20240909152042Z00'00'\", 'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1'}, page_content='was unnecessary and thus surprising readers with the relative disﬂuency of the text.\\nBoth the experts and prevailing wisdom present plausible views, but which (if either) is\\ncorrect? The present paper provides an empirical investigation of the strategy of complex-\\nity, and ﬁnds such a strategy to be unsuccessful. Five studies demonstrate that the loss of\\nﬂuency due to needless complexity in a text negatively impacts raters’ assessments of the\\ntext’s authors.\\nEXPERIMENT 1\\nExperiment 1 aimed to answer several simple questions. First, does increasing the\\ncomplexity of text succeed in making the author appear more intelligent? Second, to\\nwhat extent does the success of this strategy depend on the quality of the original, simpler\\nwriting? Finally, if the strategy is unsuccessful, is the failure of the strategy due to loss of\\nﬂuency? To answer these questions, graduate school admission essays were made more\\ncomplex by substituting some of the original words with their longest applicable thesaurus\\nentries.\\n140 D. M. Oppenheimer\\nCopyright # 2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139–156 (2006)'),\n",
       " Document(metadata={'moddate': \"D:20240910141854Z00'00'\", 'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'creator': 'Preview', 'page_label': '1', 'page': 0, 'author': 'Thu Vu', 'creationdate': \"D:20240909152042Z00'00'\", 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology', 'total_pages': 3}, page_content='be inﬂuenced by the irrelevant source of ﬂuency, they over-compensate and are biased in the opposite\\ndirection. Implications and applications are discussed. Copyright # 2005 John Wiley & Sons, Ltd.\\nWhen it comes to writing, most experts agree that clarity, simplicity and parsimony are\\nideals that authors should strive for. In their classic manual of style, Strunk and White\\n(1979) encourage authors to ‘omit needless words.’ Daryl Bem’s (1995) guidelines for\\nsubmission to Psychological Bulletin advise, ‘the ﬁrst step towards clarity is writing\\nsimply.’ Even the APA publication manual (1996) recommends, ‘direct, declarative\\nsentences with simple common words are usually best.’\\nHowever, most of us can likely recall having read papers, either by colleagues or\\nstudents, in which the author appears to be deliberately using overly complex words.\\nExperience suggests that the experts’ advice contrasts with prevailing wisdom on how to\\nsound more intelligent as a writer. In fact, when 110 Stanford undergraduates were polled\\nabout their writing habits, most of them admitted that they had made their writing more\\ncomplex in order to appear smarter. For example, when asked, ‘Have you ever changed the\\nwords in an academic essay to make the essay sound more valid or intelligent by using\\ncomplicated language?’ 86.4% of the sample admitted to having done so. Nearly two-\\nthirds answered yes to the question, ‘When you write an essay, do you turn to the thesaurus'),\n",
       " Document(metadata={'creationdate': \"D:20240909152042Z00'00'\", 'creator': 'Preview', 'moddate': \"D:20240910141854Z00'00'\", 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'page_label': '3', 'page': 2, 'author': 'Thu Vu', 'total_pages': 3, 'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology'}, page_content='While word length is not perfectly interchangeable with sentence complexity—for\\nexample, complexity can come from grammatical structure or infrequent words as\\nwell—it is a useful proxy. Using length as a manipulation of complexity allows for a\\nsimple, easily replicable word replacement algorithm. By keeping content constant and\\nvarying the complexity of vocabulary, it was possible to investigate the effectiveness of\\ncomplexity.\\nParticipants and procedure\\nSeventy-one Stanford University undergraduates participated to fulﬁl part of a course\\nrequirement. The survey was included in a packet of unrelated one-page questionnaires.\\nPackets were distributed in class, and participants were given a week to complete the entire\\npacket.\\nStimuli and design\\nSix personal statements for admissions to graduate studies in English Literature were\\ndownloaded from writing improvement websites. The essays varied greatly both in content\\nand quality of writing. Logical excerpts ranging from 138 to 253 words in length were then\\ntaken from each essay. A ‘highly complex’ version of each excerpt was prepared by\\nreplacing every noun, verb and adjective with its longest entry in the Microsoft Word 2000\\nthesaurus. Words that were longer than any thesaurus entry, were not listed in the\\nthesaurus, or for which there was no entry with the same linguistic sense were not\\nreplaced. If two entries were of the same length, the replacement was chosen alphabe-'),\n",
       " Document(metadata={'total_pages': 3, 'page': 1, 'creationdate': \"D:20240909152042Z00'00'\", 'page_label': '2', 'author': 'Thu Vu', 'producer': 'macOS Version 14.4.1 (Build 23E224) Quartz PDFContext, AppendMode 1.1', 'source': 'data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf', 'creator': 'Preview', 'moddate': \"D:20240910141854Z00'00'\", 'title': 'Oppenheimer-2006-Applied_Cognitive_Psychology'}, page_content='to draw conclusions about the effectiveness of a strategy of complexity from this data.\\nThe study did not look at how readers of the texts containing the long words perceived\\nthe authors’ intelligence. Thus, it is possi ble that although students using complex\\nvocabularies are objectively very knowledgeable, they might nonetheless be perceived\\nas being less so.\\nWhy might we believe that the experts might be correct in recommending simplicity in\\nwriting? One theory that predicts the effectiveness of straightforward writing is that of\\nprocessing ﬂuency. Simpler writing is easier to process, and studies have demonstrated\\nthat processing ﬂuency is associated with a variety of positive dimensions. Fluency leads\\nto higher judgements of truth (Reber & Schwarz, 1999), conﬁdence (Norwick & Epley,\\n2002), frequency (Tversky & Kahneman, 1973), fame (Jacoby, Kelley, Brown, &\\nJasechko, 1989), and even liking (Reber, Winkielman, & Schwarz, 1998). Furthermore,\\nthe effects of ﬂuency are strongest when the ﬂuency is discrepant—when the amount of\\nexperienced ﬂuency is surprising (Whittlesea & Williams, 2001a, 2001b). As such, it\\nwould not be surprising if the lower ﬂuency of overly complex texts caused readers to have\\nnegative evaluations of those texts and the associated authors, especially if the complexity\\nwas unnecessary and thus surprising readers with the relative disﬂuency of the text.\\nBoth the experts and prevailing wisdom present plausible views, but which (if either) is')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create retriever and get relevant chunks\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "relevant_chunks = retriever.invoke(\"What is the title of the paper?\")\n",
    "relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1633fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer\n",
    "the question. If you don't know the answer, say that you\n",
    "don't know. DON'T MAKE UP ANYTHING.\n",
    "\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70547d",
   "metadata": {},
   "source": [
    "## 3.Generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0ea37d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "You are an assistant for question-answering tasks.\n",
      "Use the following pieces of retrieved context to answer\n",
      "the question. If you don't know the answer, say that you\n",
      "don't know. DON'T MAKE UP ANYTHING.\n",
      "\n",
      "\n",
      "was unnecessary and thus surprising readers with the relative disﬂuency of the text.\n",
      "Both the experts and prevailing wisdom present plausible views, but which (if either) is\n",
      "correct? The present paper provides an empirical investigation of the strategy of complex-\n",
      "ity, and ﬁnds such a strategy to be unsuccessful. Five studies demonstrate that the loss of\n",
      "ﬂuency due to needless complexity in a text negatively impacts raters’ assessments of the\n",
      "text’s authors.\n",
      "EXPERIMENT 1\n",
      "Experiment 1 aimed to answer several simple questions. First, does increasing the\n",
      "complexity of text succeed in making the author appear more intelligent? Second, to\n",
      "what extent does the success of this strategy depend on the quality of the original, simpler\n",
      "writing? Finally, if the strategy is unsuccessful, is the failure of the strategy due to loss of\n",
      "ﬂuency? To answer these questions, graduate school admission essays were made more\n",
      "complex by substituting some of the original words with their longest applicable thesaurus\n",
      "entries.\n",
      "140 D. M. Oppenheimer\n",
      "Copyright # 2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139–156 (2006)\n",
      "\n",
      "---\n",
      "\n",
      "be inﬂuenced by the irrelevant source of ﬂuency, they over-compensate and are biased in the opposite\n",
      "direction. Implications and applications are discussed. Copyright # 2005 John Wiley & Sons, Ltd.\n",
      "When it comes to writing, most experts agree that clarity, simplicity and parsimony are\n",
      "ideals that authors should strive for. In their classic manual of style, Strunk and White\n",
      "(1979) encourage authors to ‘omit needless words.’ Daryl Bem’s (1995) guidelines for\n",
      "submission to Psychological Bulletin advise, ‘the ﬁrst step towards clarity is writing\n",
      "simply.’ Even the APA publication manual (1996) recommends, ‘direct, declarative\n",
      "sentences with simple common words are usually best.’\n",
      "However, most of us can likely recall having read papers, either by colleagues or\n",
      "students, in which the author appears to be deliberately using overly complex words.\n",
      "Experience suggests that the experts’ advice contrasts with prevailing wisdom on how to\n",
      "sound more intelligent as a writer. In fact, when 110 Stanford undergraduates were polled\n",
      "about their writing habits, most of them admitted that they had made their writing more\n",
      "complex in order to appear smarter. For example, when asked, ‘Have you ever changed the\n",
      "words in an academic essay to make the essay sound more valid or intelligent by using\n",
      "complicated language?’ 86.4% of the sample admitted to having done so. Nearly two-\n",
      "thirds answered yes to the question, ‘When you write an essay, do you turn to the thesaurus\n",
      "\n",
      "---\n",
      "\n",
      "While word length is not perfectly interchangeable with sentence complexity—for\n",
      "example, complexity can come from grammatical structure or infrequent words as\n",
      "well—it is a useful proxy. Using length as a manipulation of complexity allows for a\n",
      "simple, easily replicable word replacement algorithm. By keeping content constant and\n",
      "varying the complexity of vocabulary, it was possible to investigate the effectiveness of\n",
      "complexity.\n",
      "Participants and procedure\n",
      "Seventy-one Stanford University undergraduates participated to fulﬁl part of a course\n",
      "requirement. The survey was included in a packet of unrelated one-page questionnaires.\n",
      "Packets were distributed in class, and participants were given a week to complete the entire\n",
      "packet.\n",
      "Stimuli and design\n",
      "Six personal statements for admissions to graduate studies in English Literature were\n",
      "downloaded from writing improvement websites. The essays varied greatly both in content\n",
      "and quality of writing. Logical excerpts ranging from 138 to 253 words in length were then\n",
      "taken from each essay. A ‘highly complex’ version of each excerpt was prepared by\n",
      "replacing every noun, verb and adjective with its longest entry in the Microsoft Word 2000\n",
      "thesaurus. Words that were longer than any thesaurus entry, were not listed in the\n",
      "thesaurus, or for which there was no entry with the same linguistic sense were not\n",
      "replaced. If two entries were of the same length, the replacement was chosen alphabe-\n",
      "\n",
      "---\n",
      "\n",
      "to draw conclusions about the effectiveness of a strategy of complexity from this data.\n",
      "The study did not look at how readers of the texts containing the long words perceived\n",
      "the authors’ intelligence. Thus, it is possi ble that although students using complex\n",
      "vocabularies are objectively very knowledgeable, they might nonetheless be perceived\n",
      "as being less so.\n",
      "Why might we believe that the experts might be correct in recommending simplicity in\n",
      "writing? One theory that predicts the effectiveness of straightforward writing is that of\n",
      "processing ﬂuency. Simpler writing is easier to process, and studies have demonstrated\n",
      "that processing ﬂuency is associated with a variety of positive dimensions. Fluency leads\n",
      "to higher judgements of truth (Reber & Schwarz, 1999), conﬁdence (Norwick & Epley,\n",
      "2002), frequency (Tversky & Kahneman, 1973), fame (Jacoby, Kelley, Brown, &\n",
      "Jasechko, 1989), and even liking (Reber, Winkielman, & Schwarz, 1998). Furthermore,\n",
      "the effects of ﬂuency are strongest when the ﬂuency is discrepant—when the amount of\n",
      "experienced ﬂuency is surprising (Whittlesea & Williams, 2001a, 2001b). As such, it\n",
      "would not be surprising if the lower ﬂuency of overly complex texts caused readers to have\n",
      "negative evaluations of those texts and the associated authors, especially if the complexity\n",
      "was unnecessary and thus surprising readers with the relative disﬂuency of the text.\n",
      "Both the experts and prevailing wisdom present plausible views, but which (if either) is\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is the name of the author?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate context text\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, \n",
    "                                question=\"What is the name of the author?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dfaa4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The name of the author is D. M. Oppenheimer.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 1320, 'total_tokens': 1335, 'completion_time': 0.019747276, 'prompt_time': 0.109632318, 'queue_time': 0.0895196, 'total_time': 0.129379594}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9e1e8f8435', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2f9f6fea-ef19-40ee-9c3c-e74667646360-0', usage_metadata={'input_tokens': 1320, 'output_tokens': 15, 'total_tokens': 1335})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c4040",
   "metadata": {},
   "source": [
    "## Generate structured responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f513ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class AnswerWithSources(BaseModel):\n",
    "    \"\"\"An answer to the question, with sources and reasoning.\"\"\"\n",
    "    answer: str = Field(description=\"Answer to question\")\n",
    "    sources: str = Field(description=\"Full direct text chunk from the context used to answer the question\")\n",
    "    reasoning: str = Field(description=\"Explain the reasoning of the answer based on the sources\")\n",
    "    \n",
    "class ExtractedInfo(BaseModel):\n",
    "    \"\"\"Extracted information about the research article\"\"\"\n",
    "    paper_title: AnswerWithSources\n",
    "    paper_summary: AnswerWithSources\n",
    "    publication_year: AnswerWithSources\n",
    "    paper_authors: AnswerWithSources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0576fb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractedInfo(paper_title=AnswerWithSources(answer='The Effect of Complexity on Judgments of Authors', sources='The study did not look at how readers of the texts containing the long words perceived the authors’ intelligence.', reasoning='The title of the paper is not explicitly stated, but based on the content, it can be inferred to be related to the effect of complexity on judgments of authors.'), paper_summary=AnswerWithSources(answer='The paper investigates the strategy of using complex language to appear more intelligent and finds it to be unsuccessful.', sources='Five studies demonstrate that the loss of fluency due to needless complexity in a text negatively impacts raters’ assessments of the text’s authors.', reasoning='The paper presents an empirical investigation of the strategy of complexity and finds that it is unsuccessful in making the author appear more intelligent.'), publication_year=AnswerWithSources(answer='2006', sources='Copyright # 2005 John Wiley & Sons, Ltd. Appl. Cognit. Psychol. 20: 139–156 (2006)', reasoning='The publication year is mentioned in the text as 2006.'), paper_authors=AnswerWithSources(answer='D. M. Oppenheimer', sources='140 D. M. Oppenheimer', reasoning='The author of the paper is mentioned in the text as D. M. Oppenheimer.'))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm.with_structured_output(ExtractedInfo, strict=True)\n",
    "        )\n",
    "\n",
    "rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd4564b",
   "metadata": {},
   "source": [
    "## Transform response into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e509374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_27072\\1998849655.py:2: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  df = pd.DataFrame([structured_response.dict()])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_summary</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>paper_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>The Effects of Complexity on Writing</td>\n",
       "      <td>The paper investigates the strategy of complex...</td>\n",
       "      <td>2006</td>\n",
       "      <td>D. M. Oppenheimer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>The study did not look at how readers of the t...</td>\n",
       "      <td>Five studies demonstrate that the loss of flue...</td>\n",
       "      <td>Copyright # 2005 John Wiley &amp; Sons, Ltd. Appl....</td>\n",
       "      <td>140 D. M. Oppenheimer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasoning</th>\n",
       "      <td>The title is not explicitly stated, but based ...</td>\n",
       "      <td>The paper presents several studies that demons...</td>\n",
       "      <td>The publication year is mentioned in the text ...</td>\n",
       "      <td>The author is mentioned in the text as D. M. O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 paper_title  \\\n",
       "answer                  The Effects of Complexity on Writing   \n",
       "source     The study did not look at how readers of the t...   \n",
       "reasoning  The title is not explicitly stated, but based ...   \n",
       "\n",
       "                                               paper_summary  \\\n",
       "answer     The paper investigates the strategy of complex...   \n",
       "source     Five studies demonstrate that the loss of flue...   \n",
       "reasoning  The paper presents several studies that demons...   \n",
       "\n",
       "                                            publication_year  \\\n",
       "answer                                                  2006   \n",
       "source     Copyright # 2005 John Wiley & Sons, Ltd. Appl....   \n",
       "reasoning  The publication year is mentioned in the text ...   \n",
       "\n",
       "                                               paper_authors  \n",
       "answer                                     D. M. Oppenheimer  \n",
       "source                                 140 D. M. Oppenheimer  \n",
       "reasoning  The author is mentioned in the text as D. M. O...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_response = rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")\n",
    "df = pd.DataFrame([structured_response.dict()])\n",
    "\n",
    "# Transforming into a table with two rows: 'answer' and 'source'\n",
    "answer_row = []\n",
    "source_row = []\n",
    "reasoning_row = []\n",
    "\n",
    "for col in df.columns:\n",
    "    answer_row.append(df[col][0]['answer'])\n",
    "    source_row.append(df[col][0]['sources'])\n",
    "    reasoning_row.append(df[col][0]['reasoning'])\n",
    "\n",
    "# Create new dataframe with two rows: 'answer' and 'source'\n",
    "structured_response_df = pd.DataFrame([answer_row, source_row, reasoning_row], columns=df.columns, index=['answer', 'source', 'reasoning'])\n",
    "structured_response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a8f234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
